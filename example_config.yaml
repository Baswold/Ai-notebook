# Completeness Loop Configuration
# ================================

completeness_loop_config:
  
  # Model Configuration
  model:
    name: "devstral"
    backend: "ollama"
    max_tokens: 4096
    temperature: 0.7
    base_url: null
  
  # Limits
  limits:
    max_iterations: 50
    max_runtime_hours: 12
    max_commits: 200
    completion_threshold: 95
  
  # Agent Prompts
  agents:
    agent1_system_prompt: null
    agent2_implementation_prompt: null
    agent2_testing_prompt: null
    agent1_context_token_limit: 32000
    agent2_context_token_limit: 32000
    testing_phase_threshold: 70
  
  # Monitoring
  monitoring:
    log_level: "INFO"
    token_tracking: true
    log_file: "completeness_loop.log"


# =============================================================================
# CONFIGURATION REFERENCE
# =============================================================================
#
# MODEL BACKENDS:
#   ollama    - Local LLM server (brew install ollama && ollama pull devstral)
#   lmstudio  - LM Studio GUI (https://lmstudio.ai)
#   mlx       - Native Apple Silicon (pip install mlx-lm)
#   openai    - Any OpenAI-compatible API (set base_url)
#
# AGENT2 DUAL PROMPTS:
#   The system uses TWO different prompts for Agent 2:
#   
#   1. agent2_implementation_prompt (used when score < testing_phase_threshold)
#      - Reviews codebase against spec
#      - Focuses on implementation completeness
#      - Tells Agent 1 what features to build
#   
#   2. agent2_testing_prompt (used when score >= testing_phase_threshold)
#      - Reviews test suite quality
#      - Focuses on test coverage and edge cases
#      - Tells Agent 1 what tests to write
#
# PHASE TRANSITION:
#   testing_phase_threshold: 70  # Switch to testing phase at 70% complete
#
# CUSTOM PROMPTS:
#   agent1_system_prompt: "prompts/agent1_system.txt"
#   agent2_implementation_prompt: "prompts/agent2_implementation.txt"
#   agent2_testing_prompt: "prompts/agent2_testing.txt"
